{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059826b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "JAX RANDOM FOREST FOR NAO EEG CONTROL\n",
      "======================================================================\n",
      "JAX version: 0.7.2\n",
      "Device: CPU\n",
      "Author: Yash272001\n",
      "======================================================================\n",
      "\n",
      "Loading EEG data...\n",
      "  Processed 200 files...\n",
      "  Processed 400 files...\n",
      "  Processed 600 files...\n",
      "  Processed 800 files...\n",
      "  Processed 1000 files...\n",
      "  Processed 1200 files...\n",
      "Loaded 1203 files, 192,480 samples\n",
      "Dataset shape: X=(192480, 32), y=(192480,)\n",
      "Classes: [np.str_('backward'), np.str_('forward'), np.str_('landing'), np.str_('left'), np.str_('right'), np.str_('takeoff')]\n",
      "\n",
      "Splitting data...\n",
      "Train: 134,736 | Val: 19,248 | Test: 38,496\n",
      "\n",
      "Training Random Forest\n",
      "Trees: 40, Max depth: 8\n",
      "Bootstrap: 0.6, Feature subsample: 0.2\n",
      "\n",
      "Tree 1/40...\n",
      "   Train: 71.60% | Val: 71.28%\n",
      "Tree 10/40...\n",
      "   Train: 93.08% | Val: 93.12%\n",
      "Tree 20/40...\n",
      "   Train: 93.79% | Val: 93.71%\n",
      "Tree 30/40...\n",
      "   Train: 94.34% | Val: 94.25%\n",
      "Tree 40/40...\n",
      "   Train: 94.79% | Val: 94.72%\n",
      "\n",
      "Training complete\n",
      "\n",
      "======================================================================\n",
      "EVALUATION\n",
      "======================================================================\n",
      "Training time: 1549.7s (25.8 min)\n",
      "Test accuracy: 94.36%\n",
      "NAO control success rate: 94.36%\n",
      "\n",
      "Generating per-class metrics plot...\n",
      "Saved: C:\\Users\\yaskk\\JAX Random Forest NAO Control\\Professor Data\\data\\data\\output\\plots\\per_class_metrics.png\n",
      "Generating enhanced confusion matrix...\n",
      "Saved: C:\\Users\\yaskk\\JAX Random Forest NAO Control\\Professor Data\\data\\data\\output\\plots\\confusion_matrix_enhanced.png\n",
      "Generating training curves (dual plot)...\n",
      "Saved: C:\\Users\\yaskk\\JAX Random Forest NAO Control\\Professor Data\\data\\data\\output\\plots\\training_curves_dual.png\n",
      "Generating comprehensive training history...\n",
      "Saved: C:\\Users\\yaskk\\JAX Random Forest NAO Control\\Professor Data\\data\\data\\output\\plots\\training_history_comprehensive.png\n",
      "\n",
      "======================================================================\n",
      "CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class        Precision    Recall       F1           Support     \n",
      "----------------------------------------------------------------------\n",
      "backward          0.9003      0.9490      0.9240        6249\n",
      "forward           0.9373      0.9258      0.9315        6254\n",
      "landing           0.9116      0.9767      0.9430        6770\n",
      "left              0.9678      0.9261      0.9465        6387\n",
      "right             0.9790      0.9366      0.9573        6463\n",
      "takeoff           0.9745      0.9456      0.9598        6373\n",
      "----------------------------------------------------------------------\n",
      "Overall                                                   38496\n",
      "Accuracy          0.9436\n",
      "======================================================================\n",
      "\n",
      "Model saved: C:\\Users\\yaskk\\JAX Random Forest NAO Control\\Professor Data\\data\\data\\output\\jax_random_forest_model.pkl\n",
      "\n",
      "======================================================================\n",
      "COMPLETE\n",
      "======================================================================\n",
      "Generated plots:\n",
      "  1. per_class_metrics.png\n",
      "  2. confusion_matrix_enhanced.png\n",
      "  3. training_curves_dual.png\n",
      "  4. training_history_comprehensive.png\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from jax import random, vmap, jit, lax\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"JAX RANDOM FOREST FOR NAO EEG CONTROL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Device: {jax.default_backend().upper()}\")\n",
    "print(f\"Author: Yash272001\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration\n",
    "BASE_DATA_PATH = r\"C:\\Users\\yaskk\\JAX Random Forest NAO Control\\Professor Data\\data\\data\"\n",
    "OUTPUT_PATH = os.path.join(BASE_DATA_PATH, 'output')\n",
    "PLOTS_PATH = os.path.join(OUTPUT_PATH, 'plots')\n",
    "\n",
    "N_TREES = 40\n",
    "MAX_DEPTH = 8\n",
    "MIN_SAMPLES_SPLIT = 100\n",
    "MIN_SAMPLES_LEAF = 50\n",
    "BOOTSTRAP_RATIO = 0.6\n",
    "FEATURE_SUBSAMPLE_RATIO = 0.2\n",
    "N_THRESHOLD_SAMPLES = 4\n",
    "SEED = 42\n",
    "DESIRED_ROWS = 160\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", False)\n",
    "\n",
    "# Set matplotlib style with fallback\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except:\n",
    "    plt.style.use('default')\n",
    "\n",
    "# ==================================================\n",
    "# DATA PROCESSING\n",
    "# ==================================================\n",
    "\n",
    "def normalize_class_label(label):\n",
    "    \"\"\"Normalize class labels to standard format\"\"\"\n",
    "    label_lower = label.lower()\n",
    "    if 'backward' in label_lower:\n",
    "        return 'backward'\n",
    "    elif 'forward' in label_lower:\n",
    "        return 'forward'\n",
    "    elif 'landing' in label_lower:\n",
    "        return 'landing'\n",
    "    elif 'left' in label_lower:\n",
    "        return 'left'\n",
    "    elif 'right' in label_lower:\n",
    "        return 'right'\n",
    "    elif 'take' in label_lower or 'takeoff' in label_lower:\n",
    "        return 'takeoff'\n",
    "    return label\n",
    "\n",
    "def read_csv_flexible(file_path):\n",
    "    \"\"\"Try multiple separators to read CSV\"\"\"\n",
    "    for sep in [',', '\\t', r'\\s+']:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=sep)\n",
    "            if not df.empty and df.shape[1] > 1:\n",
    "                return df\n",
    "        except:\n",
    "            continue\n",
    "    try:\n",
    "        return pd.read_csv(file_path, header=None)\n",
    "    except:\n",
    "        raise ValueError(f\"Could not parse CSV: {file_path}\")\n",
    "\n",
    "def load_and_process_data(base_path, desired_rows):\n",
    "    \"\"\"Load and process EEG data from CSV files\"\"\"\n",
    "    print(\"\\nLoading EEG data...\")\n",
    "    \n",
    "    all_samples = []\n",
    "    all_labels = []\n",
    "    successful_files = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if 'output' in root:\n",
    "            continue\n",
    "        csv_files = [f for f in files if f.endswith('.csv')]\n",
    "        if csv_files:\n",
    "            class_label_raw = os.path.basename(root)\n",
    "            if class_label_raw.startswith(('group', 'individual', 'Test')):\n",
    "                continue\n",
    "            class_label = normalize_class_label(class_label_raw)\n",
    "            \n",
    "            for csv_file in csv_files:\n",
    "                try:\n",
    "                    file_path = os.path.join(root, csv_file)\n",
    "                    df = read_csv_flexible(file_path)\n",
    "                    if df.empty:\n",
    "                        continue\n",
    "                    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "                    if len(numeric_cols) == 0:\n",
    "                        continue\n",
    "                    df = df[numeric_cols]\n",
    "                    \n",
    "                    if len(df) < desired_rows:\n",
    "                        padding = pd.DataFrame(0, index=range(desired_rows - len(df)), \n",
    "                                             columns=df.columns)\n",
    "                        df = pd.concat([df, padding], ignore_index=True)\n",
    "                    elif len(df) > desired_rows:\n",
    "                        df = df.iloc[:desired_rows]\n",
    "                    \n",
    "                    df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "                    \n",
    "                    for idx, row in df.iterrows():\n",
    "                        all_samples.append(row.values.astype(np.float32))\n",
    "                        all_labels.append(class_label)\n",
    "                    \n",
    "                    successful_files += 1\n",
    "                    if successful_files % 200 == 0:\n",
    "                        print(f\"  Processed {successful_files} files...\")\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    print(f\"Loaded {successful_files} files, {len(all_samples):,} samples\")\n",
    "    \n",
    "    max_features = max(len(s) for s in all_samples)\n",
    "    X_list = []\n",
    "    for sample in all_samples:\n",
    "        if len(sample) < max_features:\n",
    "            padded = np.zeros(max_features, dtype=np.float32)\n",
    "            padded[:len(sample)] = sample\n",
    "            X_list.append(padded)\n",
    "        else:\n",
    "            X_list.append(sample[:max_features])\n",
    "    \n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(all_labels)\n",
    "    \n",
    "    class_names = sorted(np.unique(y))\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(class_names)}\n",
    "    y_encoded = np.array([label_to_idx[label] for label in y], dtype=np.int32)\n",
    "    \n",
    "    print(f\"Dataset shape: X={X.shape}, y={y_encoded.shape}\")\n",
    "    print(f\"Classes: {class_names}\\n\")\n",
    "    \n",
    "    return X, y_encoded, class_names, label_to_idx\n",
    "\n",
    "# ==================================================\n",
    "# JAX FUNCTIONS\n",
    "# ==================================================\n",
    "\n",
    "@partial(jit, static_argnums=(3, 4))\n",
    "def compute_weighted_gini_jax(X_col, y, threshold, n_classes, min_leaf):\n",
    "    \"\"\"Calculate weighted Gini impurity for a split\"\"\"\n",
    "    left_mask = X_col <= threshold\n",
    "    n_total = X_col.shape[0]\n",
    "    n_left = jnp.sum(left_mask)\n",
    "    n_right = n_total - n_left\n",
    "    \n",
    "    too_small = (n_left < min_leaf) | (n_right < min_leaf)\n",
    "    penalty = jnp.where(too_small, 1e9, 0.0)\n",
    "    \n",
    "    def safe_gini(mask, labels):\n",
    "        n = jnp.sum(mask)\n",
    "        safe_n = jnp.maximum(n, 1)\n",
    "        onehot = jnn.one_hot(labels, n_classes)\n",
    "        mask_expanded = mask[:, jnp.newaxis]\n",
    "        masked_onehot = onehot * mask_expanded\n",
    "        counts = jnp.sum(masked_onehot, axis=0)\n",
    "        probs = counts / safe_n\n",
    "        gini = 1.0 - jnp.sum(probs * probs)\n",
    "        return jnp.where(n > 0, gini, 1.0)\n",
    "    \n",
    "    left_gini = safe_gini(left_mask, y)\n",
    "    right_gini = safe_gini(jnp.logical_not(left_mask), y)\n",
    "    \n",
    "    weighted = (n_left / n_total) * left_gini + (n_right / n_total) * right_gini\n",
    "    \n",
    "    return weighted + penalty\n",
    "\n",
    "def find_best_split_jax(X, y, n_classes, min_leaf, key):\n",
    "    \"\"\"Find best feature and threshold for splitting using JAX\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_try = max(1, int(n_features * FEATURE_SUBSAMPLE_RATIO))\n",
    "    \n",
    "    X_jax = jnp.array(X)\n",
    "    y_jax = jnp.array(y)\n",
    "    \n",
    "    key, subkey = random.split(key)\n",
    "    feats = random.choice(subkey, n_features, (n_try,), replace=False)\n",
    "    \n",
    "    def best_for_feat(fidx):\n",
    "        col = X_jax[:, fidx]\n",
    "        sorted_col = jnp.sort(col)\n",
    "        step = jnp.maximum(1, n_samples // N_THRESHOLD_SAMPLES)\n",
    "        thr_ix = jnp.arange(step, n_samples, step)[:N_THRESHOLD_SAMPLES]\n",
    "        thrs = jnp.take(sorted_col, thr_ix)\n",
    "        \n",
    "        imp = vmap(lambda t: compute_weighted_gini_jax(col, y_jax, t, n_classes, min_leaf))(thrs)\n",
    "        \n",
    "        k = jnp.argmin(imp)\n",
    "        return imp[k], thrs[k], fidx\n",
    "    \n",
    "    imps, thrs, fids = vmap(best_for_feat)(feats)\n",
    "    \n",
    "    k = jnp.argmin(imps)\n",
    "    return int(fids[k]), float(thrs[k]), float(imps[k])\n",
    "\n",
    "# ==================================================\n",
    "# DECISION TREE\n",
    "# ==================================================\n",
    "\n",
    "class JAXDecisionTreeParams:\n",
    "    \"\"\"Decision tree structure storage\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nodes = []\n",
    "    \n",
    "    def add_leaf(self, class_label, samples):\n",
    "        node_id = len(self.nodes)\n",
    "        self.nodes.append({\n",
    "            'is_leaf': True,\n",
    "            'class': int(class_label),\n",
    "            'samples': int(samples),\n",
    "            'node_id': node_id,\n",
    "            'left': node_id,\n",
    "            'right': node_id\n",
    "        })\n",
    "        return node_id\n",
    "    \n",
    "    def add_split(self, feature, threshold, left, right, samples):\n",
    "        node_id = len(self.nodes)\n",
    "        self.nodes.append({\n",
    "            'is_leaf': False,\n",
    "            'feature': int(feature),\n",
    "            'threshold': float(threshold),\n",
    "            'left': left,\n",
    "            'right': right,\n",
    "            'samples': int(samples),\n",
    "            'node_id': node_id\n",
    "        })\n",
    "        return node_id\n",
    "\n",
    "def build_jax_tree(X, y, n_classes, max_depth, key):\n",
    "    \"\"\"Build decision tree using JAX-accelerated split finding\"\"\"\n",
    "    tree_params = JAXDecisionTreeParams()\n",
    "    \n",
    "    def build_node(X_node, y_node, depth, node_key):\n",
    "        n_samples = X_node.shape[0]\n",
    "        \n",
    "        if depth >= max_depth or n_samples < MIN_SAMPLES_SPLIT:\n",
    "            unique_labels, counts = np.unique(y_node, return_counts=True)\n",
    "            majority = unique_labels[np.argmax(counts)]\n",
    "            return tree_params.add_leaf(majority, n_samples)\n",
    "        \n",
    "        if len(np.unique(y_node)) == 1:\n",
    "            return tree_params.add_leaf(y_node[0], n_samples)\n",
    "        \n",
    "        node_key, split_key = random.split(node_key)\n",
    "        best_feat, threshold, impurity = find_best_split_jax(\n",
    "            X_node, y_node, n_classes, MIN_SAMPLES_LEAF, split_key\n",
    "        )\n",
    "        \n",
    "        if float(impurity) >= 1e8:\n",
    "            unique_labels, counts = np.unique(y_node, return_counts=True)\n",
    "            majority = unique_labels[np.argmax(counts)]\n",
    "            return tree_params.add_leaf(majority, n_samples)\n",
    "        \n",
    "        split_mask = X_node[:, best_feat] <= threshold\n",
    "        X_left = X_node[split_mask]\n",
    "        y_left = y_node[split_mask]\n",
    "        X_right = X_node[np.logical_not(split_mask)]\n",
    "        y_right = y_node[np.logical_not(split_mask)]\n",
    "        \n",
    "        node_key, left_key, right_key = random.split(node_key, 3)\n",
    "        left_child = build_node(X_left, y_left, depth + 1, left_key)\n",
    "        right_child = build_node(X_right, y_right, depth + 1, right_key)\n",
    "        \n",
    "        return tree_params.add_split(best_feat, threshold, left_child, right_child, n_samples)\n",
    "    \n",
    "    root_id = build_node(np.array(X), np.array(y), 0, key)\n",
    "    return tree_params, root_id\n",
    "\n",
    "@partial(jit, static_argnums=(2,))\n",
    "def predict_single_jax(tree_arrays, x, root_id):\n",
    "    \"\"\"JIT-compiled prediction for single sample\"\"\"\n",
    "    nodes, features, thresholds, left, right, classes = tree_arrays\n",
    "    \n",
    "    def cond(carry):\n",
    "        node_idx, _ = carry\n",
    "        return nodes[node_idx] == 0\n",
    "    \n",
    "    def body(carry):\n",
    "        node_idx, _ = carry\n",
    "        goes_left = x[features[node_idx]] <= thresholds[node_idx]\n",
    "        next_idx = jnp.where(goes_left, left[node_idx], right[node_idx])\n",
    "        return (next_idx, 0)\n",
    "    \n",
    "    node_idx, _ = lax.while_loop(cond, body, (root_id, 0))\n",
    "    \n",
    "    return classes[node_idx]\n",
    "\n",
    "def tree_to_arrays(tree_params):\n",
    "    \"\"\"Convert tree to JAX arrays for efficient prediction\"\"\"\n",
    "    n_nodes = len(tree_params.nodes)\n",
    "    nodes = np.zeros(n_nodes, dtype=np.int32)\n",
    "    features = np.zeros(n_nodes, dtype=np.int32)\n",
    "    thresholds = np.zeros(n_nodes, dtype=np.float32)\n",
    "    left = np.zeros(n_nodes, dtype=np.int32)\n",
    "    right = np.zeros(n_nodes, dtype=np.int32)\n",
    "    classes = np.zeros(n_nodes, dtype=np.int32)\n",
    "    \n",
    "    for i, node in enumerate(tree_params.nodes):\n",
    "        if node['is_leaf']:\n",
    "            nodes[i] = 1\n",
    "            classes[i] = node['class']\n",
    "            left[i] = node['left']\n",
    "            right[i] = node['right']\n",
    "        else:\n",
    "            features[i] = node['feature']\n",
    "            thresholds[i] = node['threshold']\n",
    "            left[i] = node['left']\n",
    "            right[i] = node['right']\n",
    "    \n",
    "    return (jnp.array(nodes), jnp.array(features), jnp.array(thresholds),\n",
    "            jnp.array(left), jnp.array(right), jnp.array(classes))\n",
    "\n",
    "# ==================================================\n",
    "# RANDOM FOREST\n",
    "# ==================================================\n",
    "\n",
    "class JAXRandomForest:\n",
    "    \"\"\"Random Forest implemented in pure JAX\"\"\"\n",
    "    \n",
    "    def __init__(self, n_trees=40, max_depth=8, bootstrap_ratio=0.6, seed=42):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.bootstrap_ratio = bootstrap_ratio\n",
    "        self.seed = seed\n",
    "        self.trees = []\n",
    "        self.n_classes = None\n",
    "        self.train_scores = []\n",
    "        self.val_scores = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None, verbose=True):\n",
    "        \"\"\"Fit random forest on training data\"\"\"\n",
    "        self.n_classes = len(np.unique(y_train))\n",
    "        n_samples = X_train.shape[0]\n",
    "        n_bootstrap = int(n_samples * self.bootstrap_ratio)\n",
    "        \n",
    "        X_train_jax = jnp.array(X_train)\n",
    "        y_train_jax = jnp.array(y_train)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Training Random Forest\")\n",
    "            print(f\"Trees: {self.n_trees}, Max depth: {self.max_depth}\")\n",
    "            print(f\"Bootstrap: {self.bootstrap_ratio}, Feature subsample: {FEATURE_SUBSAMPLE_RATIO}\\n\")\n",
    "        \n",
    "        key = random.PRNGKey(self.seed)\n",
    "        track_interval = 10\n",
    "        track_points = list(range(1, self.n_trees + 1, track_interval)) + [self.n_trees]\n",
    "        track_points = sorted(set(track_points))\n",
    "        \n",
    "        for i in range(self.n_trees):\n",
    "            if verbose and ((i + 1) % 10 == 0 or i == 0):\n",
    "                print(f\"Tree {i + 1}/{self.n_trees}...\")\n",
    "            \n",
    "            key, boot_key, tree_key = random.split(key, 3)\n",
    "            indices = random.choice(boot_key, n_samples, (n_bootstrap,), replace=True)\n",
    "            \n",
    "            X_boot = X_train_jax[indices]\n",
    "            y_boot = y_train_jax[indices]\n",
    "            \n",
    "            X_boot_np = np.array(X_boot)\n",
    "            y_boot_np = np.array(y_boot)\n",
    "            \n",
    "            tree_params, root_id = build_jax_tree(\n",
    "                X_boot_np, y_boot_np, self.n_classes, self.max_depth, tree_key\n",
    "            )\n",
    "            \n",
    "            tree_arrays = tree_to_arrays(tree_params)\n",
    "            self.trees.append({\n",
    "                'params': tree_params,\n",
    "                'root': root_id,\n",
    "                'arrays': tree_arrays\n",
    "            })\n",
    "            \n",
    "            if (i + 1) in track_points:\n",
    "                train_acc = self.score(X_train, y_train)\n",
    "                self.train_scores.append(train_acc)\n",
    "                self.train_losses.append(1.0 - train_acc)\n",
    "                if X_val is not None:\n",
    "                    val_acc = self.score(X_val, y_val)\n",
    "                    self.val_scores.append(val_acc)\n",
    "                    self.val_losses.append(1.0 - val_acc)\n",
    "                    if verbose:\n",
    "                        print(f\"   Train: {train_acc*100:.2f}% | Val: {val_acc*100:.2f}%\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nTraining complete\\n\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels using majority voting\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        all_preds = np.zeros((n_samples, len(self.trees)), dtype=np.int32)\n",
    "        X_jax = jnp.array(X)\n",
    "        \n",
    "        for tree_idx, tree in enumerate(self.trees):\n",
    "            pred_fn = lambda x: predict_single_jax(tree['arrays'], x, tree['root'])\n",
    "            preds = vmap(pred_fn)(X_jax)\n",
    "            all_preds[:, tree_idx] = np.array(preds)\n",
    "        \n",
    "        final_preds = []\n",
    "        for i in range(n_samples):\n",
    "            values, counts = np.unique(all_preds[i], return_counts=True)\n",
    "            final_preds.append(values[np.argmax(counts)])\n",
    "        return np.array(final_preds, dtype=np.int32)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate accuracy\"\"\"\n",
    "        return float(np.mean(self.predict(X) == y))\n",
    "\n",
    "# ==================================================\n",
    "# VISUALIZATION\n",
    "# ==================================================\n",
    "\n",
    "def plot_per_class_metrics(y_true, y_pred, class_names, output_path):\n",
    "    \"\"\"Plot per-class performance metrics (bar chart)\"\"\"\n",
    "    print(\"Generating per-class metrics plot...\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    n_classes = len(class_names)\n",
    "    precision = np.zeros(n_classes, dtype=np.float32)\n",
    "    recall = np.zeros(n_classes, dtype=np.float32)\n",
    "    f1 = np.zeros(n_classes, dtype=np.float32)\n",
    "    \n",
    "    for class_idx in range(n_classes):\n",
    "        mask_true = y_true == class_idx\n",
    "        mask_pred = y_pred == class_idx\n",
    "        tp = np.sum(mask_true & mask_pred)\n",
    "        fp = np.sum(np.logical_not(mask_true) & mask_pred)\n",
    "        fn = np.sum(mask_true & np.logical_not(mask_pred))\n",
    "        \n",
    "        precision[class_idx] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall[class_idx] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1[class_idx] = 2 * precision[class_idx] * recall[class_idx] / (precision[class_idx] + recall[class_idx]) if (precision[class_idx] + recall[class_idx]) > 0 else 0\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    x = np.arange(n_classes)\n",
    "    width = 0.25\n",
    "    \n",
    "    bars1 = ax.bar(x - width, precision, width, label='Precision', color='#5DA5DA', alpha=0.9)\n",
    "    bars2 = ax.bar(x, recall, width, label='Recall', color='#60BD68', alpha=0.9)\n",
    "    bars3 = ax.bar(x + width, f1, width, label='F1-Score', color='#F17CB0', alpha=0.9)\n",
    "    \n",
    "    for bars in [bars1, bars2, bars3]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.2f}',\n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Classes', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Per-Class Performance Metrics', fontsize=16, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_names, rotation=0, ha='center')\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    metrics_path = os.path.join(output_path, 'per_class_metrics.png')\n",
    "    plt.savefig(metrics_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {metrics_path}\")\n",
    "\n",
    "def plot_confusion_matrix_enhanced(y_true, y_pred, class_names, output_path):\n",
    "    \"\"\"Plot enhanced confusion matrix with percentages\"\"\"\n",
    "    print(\"Generating enhanced confusion matrix...\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    n_classes = len(class_names)\n",
    "    cm = np.zeros((n_classes, n_classes), dtype=np.int32)\n",
    "    for i in range(len(y_true)):\n",
    "        cm[y_true[i], y_pred[i]] += 1\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "    \n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            if i == j:\n",
    "                color = plt.cm.Greens(cm_norm[i, j])\n",
    "            else:\n",
    "                color = plt.cm.Reds(cm_norm[i, j] * 0.8)\n",
    "            ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, facecolor=color, edgecolor='white', linewidth=2))\n",
    "            \n",
    "            text_color = 'white' if (i == j and cm_norm[i, j] > 0.5) or (i != j and cm_norm[i, j] > 0.4) else 'black'\n",
    "            ax.text(j, i, f'{cm[i, j]}\\n({cm_norm[i, j]*100:.1f}%)',\n",
    "                   ha='center', va='center', fontsize=11, fontweight='bold', color=text_color)\n",
    "    \n",
    "    ax.set_xlim(-0.5, n_classes-0.5)\n",
    "    ax.set_ylim(n_classes-0.5, -0.5)\n",
    "    ax.set_xticks(range(n_classes))\n",
    "    ax.set_yticks(range(n_classes))\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=12)\n",
    "    ax.set_yticklabels(class_names, fontsize=12)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Confusion Matrix - JAX Random Forest\\nNAO Robot EEG Control', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    cm_path = os.path.join(output_path, 'confusion_matrix_enhanced.png')\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {cm_path}\")\n",
    "    \n",
    "    return cm\n",
    "\n",
    "def plot_training_curves_dual(model, output_path):\n",
    "    \"\"\"Plot training loss and accuracy over time (dual plot)\"\"\"\n",
    "    print(\"Generating training curves (dual plot)...\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    if not model.train_scores:\n",
    "        return\n",
    "    \n",
    "    n_trees_tracked = len(model.train_scores)\n",
    "    track_interval = model.n_trees // n_trees_tracked if n_trees_tracked > 1 else 1\n",
    "    epochs = [i * track_interval for i in range(1, n_trees_tracked + 1)]\n",
    "    \n",
    "    train_loss = model.train_losses\n",
    "    train_acc = [s * 100 for s in model.train_scores]\n",
    "    test_acc = [s * 100 for s in model.val_scores] if model.val_scores else None\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    ax1.plot(epochs, train_loss, 'o-', linewidth=2, markersize=4, color='#1f77b4', label='Training Loss')\n",
    "    ax1.set_xlabel('Trees', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=15, fontweight='bold')\n",
    "    ax1.legend(fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(epochs, train_acc, 'o-', linewidth=2, markersize=4, \n",
    "             color='#2ca02c', label='Training Accuracy')\n",
    "    if test_acc:\n",
    "        ax2.plot(epochs, test_acc, 's-', linewidth=2, markersize=4, \n",
    "                color='#d62728', label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Trees', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title('Accuracy Over Time', fontsize=15, fontweight='bold')\n",
    "    ax2.legend(fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    curves_path = os.path.join(output_path, 'training_curves_dual.png')\n",
    "    plt.savefig(curves_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {curves_path}\")\n",
    "\n",
    "def plot_training_history_comprehensive(model, output_path):\n",
    "    \"\"\"Plot comprehensive training history (4-panel layout)\"\"\"\n",
    "    print(\"Generating comprehensive training history...\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    if not model.train_scores:\n",
    "        return\n",
    "    \n",
    "    n_trees_tracked = len(model.train_scores)\n",
    "    track_interval = model.n_trees // n_trees_tracked if n_trees_tracked > 1 else 1\n",
    "    epochs = [i * track_interval for i in range(1, n_trees_tracked + 1)]\n",
    "    \n",
    "    train_loss = model.train_losses\n",
    "    train_acc = [s * 100 for s in model.train_scores]\n",
    "    test_acc = [s * 100 for s in model.val_scores] if model.val_scores else None\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    ax_main = fig.add_subplot(gs[0, :])\n",
    "    ax1 = ax_main\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    line1 = ax1.plot(epochs, train_loss, 'o-', linewidth=2, markersize=3, \n",
    "                     color='#1f77b4', label='Training Loss')\n",
    "    line2 = ax2.plot(epochs, train_acc, 's-', linewidth=2, markersize=3, \n",
    "                     color='#2ca02c', label='Training Accuracy')\n",
    "    if test_acc:\n",
    "        line3 = ax2.plot(epochs, test_acc, '^-', linewidth=2, markersize=3, \n",
    "                        color='#d62728', label='Validation Accuracy')\n",
    "    \n",
    "    ax1.set_xlabel('Trees', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=13, fontweight='bold', color='#1f77b4')\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold', color='#2ca02c')\n",
    "    ax1.tick_params(axis='y', labelcolor='#1f77b4')\n",
    "    ax2.tick_params(axis='y', labelcolor='#2ca02c')\n",
    "    ax1.set_title('Training History - NAO Robot EEG Control\\n\\nLoss and Accuracy', \n",
    "                 fontsize=15, fontweight='bold')\n",
    "    \n",
    "    lines = line1 + line2 + (line3 if test_acc else [])\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc='upper right', fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    ax3.plot(epochs, train_loss, 'o-', linewidth=2, markersize=3, color='#1f77b4')\n",
    "    ax3.fill_between(epochs, train_loss, alpha=0.3, color='#1f77b4')\n",
    "    ax3.set_xlabel('Trees', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Training Loss', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Training Loss Trend', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.plot(epochs, train_acc, 's-', linewidth=2, markersize=3, \n",
    "            color='#2ca02c', label='Train')\n",
    "    ax4.fill_between(epochs, train_acc, alpha=0.3, color='#2ca02c')\n",
    "    if test_acc:\n",
    "        ax4.plot(epochs, test_acc, '^-', linewidth=2, markersize=3, \n",
    "                color='#d62728', label='Validation')\n",
    "        ax4.fill_between(epochs, test_acc, alpha=0.3, color='#d62728')\n",
    "    ax4.set_xlabel('Trees', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    ax4.legend(fontsize=11)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax5 = fig.add_subplot(gs[2, :])\n",
    "    ax5.axis('off')\n",
    "    \n",
    "    final_train_acc = model.train_scores[-1] * 100\n",
    "    final_test_acc = model.val_scores[-1] * 100 if model.val_scores else 0\n",
    "    final_train_loss = train_loss[-1]\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    TRAINING SUMMARY\n",
    "    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "    Total Trees: {model.n_trees}\n",
    "    Final Training Accuracy: {final_train_acc:.2f}%\n",
    "    Final Validation Accuracy: {final_test_acc:.2f}%\n",
    "    Final Training Loss: {final_train_loss:.4f}\n",
    "    Best Training Accuracy: {max(model.train_scores)*100:.2f}%\n",
    "    Best Validation Accuracy: {max(model.val_scores)*100 if model.val_scores else 0:.2f}%\n",
    "    \"\"\"\n",
    "    \n",
    "    ax5.text(0.5, 0.5, summary_text, transform=ax5.transAxes,\n",
    "            fontsize=12, verticalalignment='center', horizontalalignment='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3),\n",
    "            family='monospace')\n",
    "    \n",
    "    history_path = os.path.join(output_path, 'training_history_comprehensive.png')\n",
    "    plt.savefig(history_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {history_path}\")\n",
    "\n",
    "def print_classification_report(y_true, y_pred, class_names):\n",
    "    \"\"\"Print classification metrics\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n{'Class':<12} {'Precision':<12} {'Recall':<12} {'F1':<12} {'Support':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        mask_true = y_true == class_idx\n",
    "        mask_pred = y_pred == class_idx\n",
    "        tp = np.sum(mask_true & mask_pred)\n",
    "        fp = np.sum(np.logical_not(mask_true) & mask_pred)\n",
    "        fn = np.sum(mask_true & np.logical_not(mask_pred))\n",
    "        \n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "        supp = np.sum(mask_true)\n",
    "        \n",
    "        print(f\"{class_name:<12} {prec:>11.4f} {rec:>11.4f} {f1:>11.4f} {int(supp):>11}\")\n",
    "    \n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Overall':<12} {'':<12} {'':<12} {'':<12} {len(y_true):>11}\")\n",
    "    print(f\"{'Accuracy':<12} {accuracy:>11.4f}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# ==================================================\n",
    "# MAIN\n",
    "# ==================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    try:\n",
    "        os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "        os.makedirs(PLOTS_PATH, exist_ok=True)\n",
    "        \n",
    "        X, y, class_names, label_to_idx = load_and_process_data(BASE_DATA_PATH, DESIRED_ROWS)\n",
    "        \n",
    "        print(\"Splitting data...\")\n",
    "        n_samples = X.shape[0]\n",
    "        n_test = int(n_samples * 0.2)\n",
    "        n_val = int(n_samples * 0.1)\n",
    "        \n",
    "        np.random.seed(SEED)\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        train_idx = indices[:n_samples - n_test - n_val]\n",
    "        val_idx = indices[n_samples - n_test - n_val:n_samples - n_test]\n",
    "        test_idx = indices[n_samples - n_test:]\n",
    "        \n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_val, y_val = X[val_idx], y[val_idx]\n",
    "        X_test, y_test = X[test_idx], y[test_idx]\n",
    "        \n",
    "        print(f\"Train: {len(X_train):,} | Val: {len(X_val):,} | Test: {len(X_test):,}\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = JAXRandomForest(\n",
    "            n_trees=N_TREES, \n",
    "            max_depth=MAX_DEPTH, \n",
    "            bootstrap_ratio=BOOTSTRAP_RATIO, \n",
    "            seed=SEED\n",
    "        )\n",
    "        model.fit(X_train, y_train, X_val, y_val, verbose=True)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"EVALUATION\")\n",
    "        print(\"=\"*70)\n",
    "        test_acc = model.score(X_test, y_test)\n",
    "        \n",
    "        print(f\"Training time: {training_time:.1f}s ({training_time/60:.1f} min)\")\n",
    "        print(f\"Test accuracy: {test_acc*100:.2f}%\")\n",
    "        print(f\"NAO control success rate: {test_acc*100:.2f}%\\n\")\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        plot_per_class_metrics(y_test, y_pred, class_names, PLOTS_PATH)\n",
    "        cm = plot_confusion_matrix_enhanced(y_test, y_pred, class_names, PLOTS_PATH)\n",
    "        plot_training_curves_dual(model, PLOTS_PATH)\n",
    "        plot_training_history_comprehensive(model, PLOTS_PATH)\n",
    "        \n",
    "        print_classification_report(y_test, y_pred, class_names)\n",
    "        \n",
    "        model_data = {\n",
    "            'model': model,\n",
    "            'class_names': class_names,\n",
    "            'label_to_idx': label_to_idx,\n",
    "            'test_accuracy': test_acc,\n",
    "            'confusion_matrix': cm,\n",
    "            'training_time': training_time,\n",
    "            'author': 'Yash272001',\n",
    "            'date': '2025-01-19',\n",
    "            'jax_version': jax.__version__\n",
    "        }\n",
    "        \n",
    "        model_path = os.path.join(OUTPUT_PATH, 'jax_random_forest_model.pkl')\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"\\nModel saved: {model_path}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Generated plots:\")\n",
    "        print(\"  1. per_class_metrics.png\")\n",
    "        print(\"  2. confusion_matrix_enhanced.png\")\n",
    "        print(\"  3. training_curves_dual.png\")\n",
    "        print(\"  4. training_history_comprehensive.png\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
