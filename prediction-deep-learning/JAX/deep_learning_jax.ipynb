{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf11dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "JAX DEEP LEARNING FOR NAO EEG CONTROL\n",
      "======================================================================\n",
      "JAX version: 0.7.2\n",
      "Device: CPU\n",
      "Author: Yash272001\n",
      "======================================================================\n",
      "\n",
      "Loading EEG data...\n",
      "  Processed 200 files...\n",
      "  Processed 400 files...\n",
      "  Processed 600 files...\n",
      "  Processed 800 files...\n",
      "  Processed 1000 files...\n",
      "  Processed 1200 files...\n",
      "Loaded 1203 files, 192,480 samples\n",
      "Dataset shape: X=(192480, 32), y=(192480,)\n",
      "Classes: [np.str_('backward'), np.str_('forward'), np.str_('landing'), np.str_('left'), np.str_('right'), np.str_('takeoff')]\n",
      "\n",
      "Splitting data...\n",
      "Train: 134,736 | Val: 19,248 | Test: 38,496\n",
      "\n",
      "\n",
      "Training JAX Deep Learning Model\n",
      "Architecture: Input(32) -> [256, 128, 64, 32] -> Output(6)\n",
      "Optimizer: Adam (lr=0.001)\n",
      "Batch size: 256, Epochs: 100\n",
      "\n",
      "Epoch  10/100 | Loss: 0.5376 | Train: 76.01% | Val: 83.66% | Best: 85.35%\n",
      "Epoch  20/100 | Loss: 0.4294 | Train: 81.01% | Val: 88.85% | Best: 88.85%\n",
      "Epoch  30/100 | Loss: 0.3849 | Train: 82.95% | Val: 88.25% | Best: 89.84%\n",
      "Epoch  40/100 | Loss: 0.3552 | Train: 84.42% | Val: 90.57% | Best: 91.29%\n",
      "Epoch  50/100 | Loss: 0.3386 | Train: 85.18% | Val: 91.31% | Best: 91.73%\n",
      "Epoch  60/100 | Loss: 0.3260 | Train: 85.78% | Val: 91.52% | Best: 91.88%\n",
      "Epoch  70/100 | Loss: 0.3109 | Train: 86.46% | Val: 92.73% | Best: 92.77%\n",
      "Epoch  80/100 | Loss: 0.3025 | Train: 86.83% | Val: 93.08% | Best: 93.08%\n",
      "Epoch  90/100 | Loss: 0.2981 | Train: 87.18% | Val: 93.39% | Best: 93.39%\n",
      "Epoch 100/100 | Loss: 0.2936 | Train: 87.14% | Val: 93.24% | Best: 93.39%\n",
      "\n",
      "Training complete!\n",
      "Best validation accuracy: 93.39%\n",
      "======================================================================\n",
      "EVALUATION\n",
      "======================================================================\n",
      "Training time: 238.4s (4.0 min)\n",
      "Test accuracy: 93.60%\n",
      "NAO control success rate: 93.60%\n",
      "\n",
      "Generating training curves...\n",
      "Saved: C:\\Users\\yaskk\\JAX Random Forest NAO Control\\Professor Data\\data\\data\\output\\plots\\training_curves_deep_learning.png\n",
      "Generating comprehensive training history...\n",
      "Saved: C:\\Users\\yaskk\\JAX Random Forest NAO Control\\Professor Data\\data\\data\\output\\plots\\training_history_comprehensive_deep_learning.png\n",
      "Generating confusion matrix...\n",
      "Saved: C:\\Users\\yaskk\\JAX Random Forest NAO Control\\Professor Data\\data\\data\\output\\plots\\confusion_matrix_deep_learning.png\n",
      "Generating per-class metrics...\n",
      "Saved: C:\\Users\\yaskk\\JAX Random Forest NAO Control\\Professor Data\\data\\data\\output\\plots\\per_class_metrics_deep_learning.png\n",
      "\n",
      "======================================================================\n",
      "CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class        Precision    Recall       F1           Support     \n",
      "----------------------------------------------------------------------\n",
      "backward          0.9702      0.9072      0.9376        6249\n",
      "forward           0.9172      0.9547      0.9356        6254\n",
      "landing           0.8864      0.9227      0.9042        6770\n",
      "left              0.9459      0.9355      0.9406        6387\n",
      "right             0.9308      0.9491      0.9399        6463\n",
      "takeoff           0.9753      0.9470      0.9609        6373\n",
      "----------------------------------------------------------------------\n",
      "Overall                                                   38496\n",
      "Accuracy          0.9360\n",
      "======================================================================\n",
      "\n",
      "Model saved: C:\\Users\\yaskk\\JAX Random Forest NAO Control\\Professor Data\\data\\data\\output\\jax_deep_learning_model.pkl\n",
      "\n",
      "======================================================================\n",
      "COMPLETE\n",
      "======================================================================\n",
      "Generated plots:\n",
      "  1. training_curves_deep_learning.png\n",
      "  2. training_history_comprehensive_deep_learning.png\n",
      "  3. confusion_matrix_deep_learning.png\n",
      "  4. per_class_metrics_deep_learning.png\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "JAX Deep Learning Implementation for Brain-Computer Interface Applications\n",
    "===========================================================================\n",
    "\n",
    "Author: Yash Patel (GitHub: Yash272001)\n",
    "Date: January 2025\n",
    "Project: Avatar BCI Platform - Neural Interface Control Systems\n",
    "Repository: https://github.com/3C-SCSU/Avatar\n",
    "\n",
    "MAIN CONTRIBUTION:\n",
    "This implementation demonstrates a neural network approach using JAX and Flax \n",
    "for EEG-based brain-computer interface control, providing a deep learning \n",
    "alternative to the Random Forest implementation.\n",
    "\n",
    "REFERENCES:\n",
    "===========\n",
    "\n",
    "[1] Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., \n",
    "    Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., \n",
    "    Wanderman-Milne, S., & Zhang, Q. (2018). JAX: composable \n",
    "    transformations of Python+NumPy programs (Version 0.3.13). \n",
    "    http://github.com/google/jax\n",
    "    Used: jit, random, value_and_grad\n",
    "\n",
    "[2] Google Research. (2024). JAX Documentation. \n",
    "    https://jax.readthedocs.io/en/latest/\n",
    "    Used: API reference for all JAX transformations\n",
    "\n",
    "[3] Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J., \n",
    "    Buchlovsky, P., ... & Viola, F. (2020). The DeepMind JAX Ecosystem. \n",
    "    http://github.com/deepmind\n",
    "    Used: Ecosystem context\n",
    "\n",
    "[4] Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre, B., \n",
    "    Steiner, A., & van Zee, M. (2020). Flax: A neural network library \n",
    "    and ecosystem for JAX (Version 0.8.0). http://github.com/google/flax\n",
    "    Used: nn.Dense, nn.BatchNorm, nn.Dropout, nn.relu\n",
    "\n",
    "[5] Babuschkin, I., Hennigan, T., Norman, M., et al. (2020). Optax: \n",
    "    composable gradient transformation and optimisation, in JAX. \n",
    "    http://github.com/deepmind/optax\n",
    "    Used: optax.adam, optax.exponential_decay, optax.clip_by_global_norm\n",
    "\n",
    "[6] Ioffe, S., & Szegedy, C. (2015). Batch Normalization: Accelerating \n",
    "    Deep Network Training by Reducing Internal Covariate Shift. \n",
    "    Proceedings of the 32nd International Conference on Machine Learning, \n",
    "    448-456.\n",
    "    Used: nn.BatchNorm implementation in MLPClassifier\n",
    "\n",
    "[7] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & \n",
    "    Salakhutdinov, R. (2014). Dropout: A Simple Way to Prevent Neural \n",
    "    Networks from Overfitting. Journal of Machine Learning Research, \n",
    "    15(1), 1929-1958.\n",
    "    Used: nn.Dropout implementation in MLPClassifier\n",
    "\n",
    "[8] Kingma, D. P., & Ba, J. (2014). Adam: A Method for Stochastic \n",
    "    Optimization. arXiv preprint arXiv:1412.6980.\n",
    "    Used: optax.adam optimizer in create_train_state\n",
    "\n",
    "[9] Harris, C. R., Millman, K. J., van der Walt, S. J., et al. (2020). \n",
    "    Array programming with NumPy. Nature, 585(7825), 357-362. \n",
    "    https://doi.org/10.1038/s41586-020-2649-2\n",
    "    Used: np.array, np.mean, standardization\n",
    "\n",
    "[10] McKinney, W. (2010). Data structures for statistical computing in \n",
    "     python. Proceedings of the 9th Python in Science Conference, 445, 51-56.\n",
    "     Used: pd.read_csv, pd.DataFrame\n",
    "\n",
    "[11] Hunter, J. D. (2007). Matplotlib: A 2D graphics environment. \n",
    "     Computing in Science & Engineering, 9(3), 90-95.\n",
    "     Used: plt.subplots, plt.savefig\n",
    "\n",
    "[12] Sanei, S., & Chambers, J. A. (2013). EEG signal processing. \n",
    "     John Wiley & Sons.\n",
    "     Used: EEG processing context\n",
    "\n",
    "[13] Wolpaw, J. R., Birbaumer, N., McFarland, D. J., Pfurtscheller, G., \n",
    "     & Vaughan, T. M. (2002). Brain-computer interfaces for communication \n",
    "     and control. Clinical Neurophysiology, 113(6), 767-791.\n",
    "     Used: BCI principles\n",
    "\n",
    "NOTE:\n",
    "Development tools (GitHub Copilot, Claude AI, ChatGPT) were used as \n",
    "assistive coding aids, but all technical content is derived from and \n",
    "cited to the original academic sources listed above.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from typing import Sequence, Any\n",
    "from functools import partial\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"JAX DEEP LEARNING FOR NAO EEG CONTROL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Device: {jax.default_backend().upper()}\")\n",
    "print(f\"Author: Yash272001\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration\n",
    "BASE_DATA_PATH = r\"C:\\Users\\yaskk\\JAX Random Forest NAO Control\\Professor Data\\data\\data\"\n",
    "OUTPUT_PATH = os.path.join(BASE_DATA_PATH, 'output')\n",
    "PLOTS_PATH = os.path.join(OUTPUT_PATH, 'plots')\n",
    "\n",
    "HIDDEN_DIMS = [256, 128, 64, 32]\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 100\n",
    "DROPOUT_RATE = 0.1\n",
    "NOISE_AUGMENTATION = 0.01\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "GRADIENT_CLIP_NORM = 1.0\n",
    "SEED = 42\n",
    "DESIRED_ROWS = 160\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", False)\n",
    "\n",
    "# Set matplotlib style with fallback\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except:\n",
    "    plt.style.use('default')\n",
    "\n",
    "# ==================================================\n",
    "# DATA PROCESSING\n",
    "# ==================================================\n",
    "\n",
    "def normalize_class_label(label):\n",
    "    \"\"\"Normalize class labels to standard format\"\"\"\n",
    "    label_lower = label.lower()\n",
    "    if 'backward' in label_lower:\n",
    "        return 'backward'\n",
    "    elif 'forward' in label_lower:\n",
    "        return 'forward'\n",
    "    elif 'landing' in label_lower:\n",
    "        return 'landing'\n",
    "    elif 'left' in label_lower:\n",
    "        return 'left'\n",
    "    elif 'right' in label_lower:\n",
    "        return 'right'\n",
    "    elif 'take' in label_lower or 'takeoff' in label_lower:\n",
    "        return 'takeoff'\n",
    "    return label\n",
    "\n",
    "def read_csv_flexible(file_path):\n",
    "    \"\"\"Try multiple separators to read CSV\"\"\"\n",
    "    for sep in [',', '\\t', r'\\s+']:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=sep)\n",
    "            if not df.empty and df.shape[1] > 1:\n",
    "                return df\n",
    "        except:\n",
    "            continue\n",
    "    try:\n",
    "        return pd.read_csv(file_path, header=None)\n",
    "    except:\n",
    "        raise ValueError(f\"Could not parse CSV: {file_path}\")\n",
    "\n",
    "def load_and_process_data(base_path, desired_rows):\n",
    "    \"\"\"Load and process EEG data from CSV files\"\"\"\n",
    "    print(\"\\nLoading EEG data...\")\n",
    "    \n",
    "    all_samples = []\n",
    "    all_labels = []\n",
    "    successful_files = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if 'output' in root:\n",
    "            continue\n",
    "        csv_files = [f for f in files if f.endswith('.csv')]\n",
    "        if csv_files:\n",
    "            class_label_raw = os.path.basename(root)\n",
    "            if class_label_raw.startswith(('group', 'individual', 'Test')):\n",
    "                continue\n",
    "            class_label = normalize_class_label(class_label_raw)\n",
    "            \n",
    "            for csv_file in csv_files:\n",
    "                try:\n",
    "                    file_path = os.path.join(root, csv_file)\n",
    "                    df = read_csv_flexible(file_path)\n",
    "                    if df.empty:\n",
    "                        continue\n",
    "                    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "                    if len(numeric_cols) == 0:\n",
    "                        continue\n",
    "                    df = df[numeric_cols]\n",
    "                    \n",
    "                    if len(df) < desired_rows:\n",
    "                        padding = pd.DataFrame(0, index=range(desired_rows - len(df)), \n",
    "                                             columns=df.columns)\n",
    "                        df = pd.concat([df, padding], ignore_index=True)\n",
    "                    elif len(df) > desired_rows:\n",
    "                        df = df.iloc[:desired_rows]\n",
    "                    \n",
    "                    df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "                    \n",
    "                    for idx, row in df.iterrows():\n",
    "                        all_samples.append(row.values.astype(np.float32))\n",
    "                        all_labels.append(class_label)\n",
    "                    \n",
    "                    successful_files += 1\n",
    "                    if successful_files % 200 == 0:\n",
    "                        print(f\"  Processed {successful_files} files...\")\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    print(f\"Loaded {successful_files} files, {len(all_samples):,} samples\")\n",
    "    \n",
    "    max_features = max(len(s) for s in all_samples)\n",
    "    X_list = []\n",
    "    for sample in all_samples:\n",
    "        if len(sample) < max_features:\n",
    "            padded = np.zeros(max_features, dtype=np.float32)\n",
    "            padded[:len(sample)] = sample\n",
    "            X_list.append(padded)\n",
    "        else:\n",
    "            X_list.append(sample[:max_features])\n",
    "    \n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(all_labels)\n",
    "    \n",
    "    # Standardize features\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_std = np.std(X, axis=0) + 1e-8\n",
    "    X = (X - X_mean) / X_std\n",
    "    \n",
    "    class_names = sorted(np.unique(y))\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(class_names)}\n",
    "    y_encoded = np.array([label_to_idx[label] for label in y], dtype=np.int32)\n",
    "    \n",
    "    print(f\"Dataset shape: X={X.shape}, y={y_encoded.shape}\")\n",
    "    print(f\"Classes: {class_names}\\n\")\n",
    "    \n",
    "    return X, y_encoded, class_names, label_to_idx, X_mean, X_std\n",
    "\n",
    "# ==================================================\n",
    "# JAX/FLAX NEURAL NETWORK\n",
    "# ==================================================\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    \"\"\"Multi-layer perceptron with batch normalization\"\"\"\n",
    "    hidden_dims: Sequence[int]\n",
    "    n_classes: int\n",
    "    dropout_rate: float = 0.1\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool = False):\n",
    "        for i, dim in enumerate(self.hidden_dims):\n",
    "            x = nn.Dense(features=dim, name=f'dense_{i}')(x)\n",
    "            x = nn.BatchNorm(\n",
    "                use_running_average=not training,\n",
    "                momentum=0.9,\n",
    "                epsilon=1e-5,\n",
    "                name=f'bn_{i}'\n",
    "            )(x)\n",
    "            x = nn.relu(x)\n",
    "            x = nn.Dropout(rate=self.dropout_rate, deterministic=not training)(x)\n",
    "        \n",
    "        x = nn.Dense(features=self.n_classes, name='output')(x)\n",
    "        return x\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class TrainStateWithBatchStats(train_state.TrainState):\n",
    "    \"\"\"Training state with batch statistics for BatchNorm\"\"\"\n",
    "    batch_stats: Any\n",
    "\n",
    "def create_train_state(rng, input_dim, hidden_dims, n_classes):\n",
    "    \"\"\"Create training state with optimizer\"\"\"\n",
    "    model = MLPClassifier(hidden_dims=hidden_dims, n_classes=n_classes, dropout_rate=DROPOUT_RATE)\n",
    "    \n",
    "    # Initialize with training=True to create batch_stats\n",
    "    variables = model.init(rng, jnp.ones([1, input_dim]), training=True)\n",
    "    params = variables['params']\n",
    "    batch_stats = variables['batch_stats']\n",
    "    \n",
    "    schedule = optax.exponential_decay(\n",
    "        init_value=LEARNING_RATE,\n",
    "        transition_steps=1000,\n",
    "        decay_rate=0.96,\n",
    "        staircase=False\n",
    "    )\n",
    "    \n",
    "    tx = optax.chain(\n",
    "        optax.clip_by_global_norm(GRADIENT_CLIP_NORM),\n",
    "        optax.adam(schedule)\n",
    "    )\n",
    "    \n",
    "    return TrainStateWithBatchStats.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        batch_stats=batch_stats,\n",
    "        tx=tx\n",
    "    )\n",
    "\n",
    "def augment_data(batch_x, rng, noise_level=NOISE_AUGMENTATION):\n",
    "    \"\"\"Add Gaussian noise for data augmentation\"\"\"\n",
    "    noise = random.normal(rng, batch_x.shape) * noise_level\n",
    "    return batch_x + noise\n",
    "\n",
    "@partial(jax.jit, static_argnums=(4,))\n",
    "def train_step(state, batch_x, batch_y, rng, n_classes):\n",
    "    \"\"\"Training step with batch normalization\"\"\"\n",
    "    aug_rng, dropout_rng = random.split(rng)\n",
    "    batch_x_aug = augment_data(batch_x, aug_rng)\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        variables = {'params': params, 'batch_stats': state.batch_stats}\n",
    "        logits, new_model_state = state.apply_fn(\n",
    "            variables,\n",
    "            batch_x_aug,\n",
    "            training=True,\n",
    "            mutable=['batch_stats'],\n",
    "            rngs={'dropout': dropout_rng}\n",
    "        )\n",
    "        one_hot = jax.nn.one_hot(batch_y, n_classes)\n",
    "        loss = optax.softmax_cross_entropy(logits=logits, labels=one_hot).mean()\n",
    "        return loss, (logits, new_model_state)\n",
    "    \n",
    "    (loss, (logits, new_model_state)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    \n",
    "    state = state.apply_gradients(\n",
    "        grads=grads,\n",
    "        batch_stats=new_model_state['batch_stats']\n",
    "    )\n",
    "    \n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    accuracy = jnp.mean(predictions == batch_y)\n",
    "    \n",
    "    return state, loss, accuracy\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(state, batch_x, batch_y):\n",
    "    \"\"\"Evaluation step\"\"\"\n",
    "    variables = {'params': state.params, 'batch_stats': state.batch_stats}\n",
    "    logits = state.apply_fn(variables, batch_x, training=False)\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    accuracy = jnp.mean(predictions == batch_y)\n",
    "    return accuracy, predictions\n",
    "\n",
    "def create_batches(X, y, batch_size, rng):\n",
    "    \"\"\"Create shuffled batches (including last partial batch)\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    perm = random.permutation(rng, n_samples)\n",
    "    X_shuffled = X[perm]\n",
    "    y_shuffled = y[perm]\n",
    "    \n",
    "    # Include last partial batch\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        yield X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx]\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, n_epochs, batch_size):\n",
    "    \"\"\"Train the neural network\"\"\"\n",
    "    # Derive n_classes from data\n",
    "    n_classes = int(jnp.max(y_train)) + 1\n",
    "    \n",
    "    print(\"\\nTraining JAX Deep Learning Model\")\n",
    "    print(f\"Architecture: Input({X_train.shape[1]}) -> {HIDDEN_DIMS} -> Output({n_classes})\")\n",
    "    print(f\"Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "    print(f\"Batch size: {batch_size}, Epochs: {n_epochs}\\n\")\n",
    "    \n",
    "    rng = random.PRNGKey(SEED)\n",
    "    rng, init_rng = random.split(rng)\n",
    "    \n",
    "    state = create_train_state(\n",
    "        init_rng,\n",
    "        X_train.shape[1],\n",
    "        HIDDEN_DIMS,\n",
    "        n_classes\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'epochs': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_params = None\n",
    "    best_batch_stats = None\n",
    "    no_improve_count = 0\n",
    "    \n",
    "    track_interval = 10\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        rng, epoch_rng = random.split(rng)\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch_x, batch_y in create_batches(X_train, y_train, batch_size, epoch_rng):\n",
    "            rng, step_rng = random.split(rng)\n",
    "            state, loss, acc = train_step(state, batch_x, batch_y, step_rng, n_classes)\n",
    "            train_loss += loss\n",
    "            train_acc += acc\n",
    "            n_batches += 1\n",
    "        \n",
    "        # Cast JAX scalars to Python floats\n",
    "        train_loss = float(train_loss / n_batches)\n",
    "        train_acc = float(train_acc / n_batches)\n",
    "        \n",
    "        val_acc, _ = eval_step(state, X_val, y_val)\n",
    "        val_acc = float(val_acc)\n",
    "        \n",
    "        if (epoch + 1) % track_interval == 0 or epoch == 0:\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            history['epochs'].append(epoch + 1)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_params = state.params\n",
    "            best_batch_stats = state.batch_stats\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{n_epochs} | \"\n",
    "                  f\"Loss: {train_loss:.4f} | \"\n",
    "                  f\"Train: {train_acc*100:.2f}% | \"\n",
    "                  f\"Val: {val_acc*100:.2f}% | \"\n",
    "                  f\"Best: {best_val_acc*100:.2f}%\")\n",
    "        \n",
    "        if no_improve_count >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            state = state.replace(params=best_params, batch_stats=best_batch_stats)\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nTraining complete!\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "    \n",
    "    if best_params is not None:\n",
    "        state = state.replace(params=best_params, batch_stats=best_batch_stats)\n",
    "    \n",
    "    return state, best_val_acc, history\n",
    "\n",
    "# ==================================================\n",
    "# VISUALIZATION\n",
    "# ==================================================\n",
    "\n",
    "def plot_training_curves(history, output_path):\n",
    "    \"\"\"Plot training curves\"\"\"\n",
    "    print(\"Generating training curves...\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    epochs = history['epochs']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    ax1.plot(epochs, history['train_loss'], 'o-', linewidth=2, markersize=4, color='#1f77b4', label='Training Loss')\n",
    "    ax1.set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=15, fontweight='bold')\n",
    "    ax1.legend(fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(epochs, [a*100 for a in history['train_acc']], 'o-', linewidth=2, markersize=4, \n",
    "             color='#2ca02c', label='Training Accuracy')\n",
    "    ax2.plot(epochs, [a*100 for a in history['val_acc']], 's-', linewidth=2, markersize=4, \n",
    "            color='#d62728', label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title('Accuracy Over Time', fontsize=15, fontweight='bold')\n",
    "    ax2.legend(fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    curves_path = os.path.join(output_path, 'training_curves_deep_learning.png')\n",
    "    plt.savefig(curves_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {curves_path}\")\n",
    "\n",
    "def plot_training_history_comprehensive(history, output_path):\n",
    "    \"\"\"Plot comprehensive training history\"\"\"\n",
    "    print(\"Generating comprehensive training history...\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    if not history['train_acc']:\n",
    "        return\n",
    "    \n",
    "    epochs = history['epochs']\n",
    "    train_loss = history['train_loss']\n",
    "    train_acc = [s * 100 for s in history['train_acc']]\n",
    "    val_acc = [s * 100 for s in history['val_acc']] if history['val_acc'] else None\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    ax_main = fig.add_subplot(gs[0, :])\n",
    "    ax1 = ax_main\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    line1 = ax1.plot(epochs, train_loss, 'o-', linewidth=2, markersize=3, \n",
    "                     color='#1f77b4', label='Training Loss')\n",
    "    line2 = ax2.plot(epochs, train_acc, 's-', linewidth=2, markersize=3, \n",
    "                     color='#2ca02c', label='Training Accuracy')\n",
    "    if val_acc:\n",
    "        line3 = ax2.plot(epochs, val_acc, '^-', linewidth=2, markersize=3, \n",
    "                        color='#d62728', label='Validation Accuracy')\n",
    "    \n",
    "    ax1.set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=13, fontweight='bold', color='#1f77b4')\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold', color='#2ca02c')\n",
    "    ax1.tick_params(axis='y', labelcolor='#1f77b4')\n",
    "    ax2.tick_params(axis='y', labelcolor='#2ca02c')\n",
    "    ax1.set_title('Training History - NAO Robot EEG Control\\n\\nLoss and Accuracy', \n",
    "                 fontsize=15, fontweight='bold')\n",
    "    \n",
    "    lines = line1 + line2 + (line3 if val_acc else [])\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc='upper right', fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    ax3.plot(epochs, train_loss, 'o-', linewidth=2, markersize=3, color='#1f77b4')\n",
    "    ax3.fill_between(epochs, train_loss, alpha=0.3, color='#1f77b4')\n",
    "    ax3.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Training Loss', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Training Loss Trend', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.plot(epochs, train_acc, 's-', linewidth=2, markersize=3, \n",
    "            color='#2ca02c', label='Train')\n",
    "    ax4.fill_between(epochs, train_acc, alpha=0.3, color='#2ca02c')\n",
    "    if val_acc:\n",
    "        ax4.plot(epochs, val_acc, '^-', linewidth=2, markersize=3, \n",
    "                color='#d62728', label='Validation')\n",
    "        ax4.fill_between(epochs, val_acc, alpha=0.3, color='#d62728')\n",
    "    ax4.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    ax4.legend(fontsize=11)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax5 = fig.add_subplot(gs[2, :])\n",
    "    ax5.axis('off')\n",
    "    \n",
    "    final_train_acc = history['train_acc'][-1] * 100\n",
    "    final_val_acc = history['val_acc'][-1] * 100 if history['val_acc'] else 0\n",
    "    final_train_loss = train_loss[-1]\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    TRAINING SUMMARY\n",
    "    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "    Total Epochs: {len(epochs)}\n",
    "    Final Training Accuracy: {final_train_acc:.2f}%\n",
    "    Final Validation Accuracy: {final_val_acc:.2f}%\n",
    "    Final Training Loss: {final_train_loss:.4f}\n",
    "    Best Training Accuracy: {max(history['train_acc'])*100:.2f}%\n",
    "    Best Validation Accuracy: {max(history['val_acc'])*100 if history['val_acc'] else 0:.2f}%\n",
    "    \"\"\"\n",
    "    \n",
    "    ax5.text(0.5, 0.5, summary_text, transform=ax5.transAxes,\n",
    "            fontsize=12, verticalalignment='center', horizontalalignment='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3),\n",
    "            family='monospace')\n",
    "    \n",
    "    history_path = os.path.join(output_path, 'training_history_comprehensive_deep_learning.png')\n",
    "    plt.savefig(history_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {history_path}\")\n",
    "\n",
    "def plot_confusion_matrix_enhanced(y_true, y_pred, class_names, output_path):\n",
    "    \"\"\"Plot enhanced confusion matrix\"\"\"\n",
    "    print(\"Generating confusion matrix...\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    n_classes = len(class_names)\n",
    "    cm = np.zeros((n_classes, n_classes), dtype=np.int32)\n",
    "    for i in range(len(y_true)):\n",
    "        cm[y_true[i], y_pred[i]] += 1\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "    \n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            if i == j:\n",
    "                color = plt.cm.Greens(cm_norm[i, j])\n",
    "            else:\n",
    "                color = plt.cm.Reds(cm_norm[i, j] * 0.8)\n",
    "            ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, facecolor=color, edgecolor='white', linewidth=2))\n",
    "            \n",
    "            text_color = 'white' if (i == j and cm_norm[i, j] > 0.5) or (i != j and cm_norm[i, j] > 0.4) else 'black'\n",
    "            ax.text(j, i, f'{cm[i, j]}\\n({cm_norm[i, j]*100:.1f}%)',\n",
    "                   ha='center', va='center', fontsize=11, fontweight='bold', color=text_color)\n",
    "    \n",
    "    ax.set_xlim(-0.5, n_classes-0.5)\n",
    "    ax.set_ylim(n_classes-0.5, -0.5)\n",
    "    ax.set_xticks(range(n_classes))\n",
    "    ax.set_yticks(range(n_classes))\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=12)\n",
    "    ax.set_yticklabels(class_names, fontsize=12)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Confusion Matrix - JAX Deep Learning\\nNAO Robot EEG Control', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    cm_path = os.path.join(output_path, 'confusion_matrix_deep_learning.png')\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {cm_path}\")\n",
    "    \n",
    "    return cm\n",
    "\n",
    "def plot_per_class_metrics(y_true, y_pred, class_names, output_path):\n",
    "    \"\"\"Plot per-class metrics\"\"\"\n",
    "    print(\"Generating per-class metrics...\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    n_classes = len(class_names)\n",
    "    precision = np.zeros(n_classes, dtype=np.float32)\n",
    "    recall = np.zeros(n_classes, dtype=np.float32)\n",
    "    f1 = np.zeros(n_classes, dtype=np.float32)\n",
    "    \n",
    "    for class_idx in range(n_classes):\n",
    "        mask_true = y_true == class_idx\n",
    "        mask_pred = y_pred == class_idx\n",
    "        tp = np.sum(mask_true & mask_pred)\n",
    "        fp = np.sum(np.logical_not(mask_true) & mask_pred)\n",
    "        fn = np.sum(mask_true & np.logical_not(mask_pred))\n",
    "        \n",
    "        precision[class_idx] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall[class_idx] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1[class_idx] = 2 * precision[class_idx] * recall[class_idx] / (precision[class_idx] + recall[class_idx]) if (precision[class_idx] + recall[class_idx]) > 0 else 0\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    x = np.arange(n_classes)\n",
    "    width = 0.25\n",
    "    \n",
    "    bars1 = ax.bar(x - width, precision, width, label='Precision', color='#5DA5DA', alpha=0.9)\n",
    "    bars2 = ax.bar(x, recall, width, label='Recall', color='#60BD68', alpha=0.9)\n",
    "    bars3 = ax.bar(x + width, f1, width, label='F1-Score', color='#F17CB0', alpha=0.9)\n",
    "    \n",
    "    for bars in [bars1, bars2, bars3]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.2f}',\n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Classes', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Per-Class Performance Metrics', fontsize=16, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_names, rotation=0, ha='center')\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    metrics_path = os.path.join(output_path, 'per_class_metrics_deep_learning.png')\n",
    "    plt.savefig(metrics_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {metrics_path}\")\n",
    "\n",
    "def print_classification_report(y_true, y_pred, class_names):\n",
    "    \"\"\"Print classification metrics\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n{'Class':<12} {'Precision':<12} {'Recall':<12} {'F1':<12} {'Support':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        mask_true = y_true == class_idx\n",
    "        mask_pred = y_pred == class_idx\n",
    "        tp = np.sum(mask_true & mask_pred)\n",
    "        fp = np.sum(np.logical_not(mask_true) & mask_pred)\n",
    "        fn = np.sum(mask_true & np.logical_not(mask_pred))\n",
    "        \n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "        supp = np.sum(mask_true)\n",
    "        \n",
    "        print(f\"{class_name:<12} {prec:>11.4f} {rec:>11.4f} {f1:>11.4f} {int(supp):>11}\")\n",
    "    \n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Overall':<12} {'':<12} {'':<12} {'':<12} {len(y_true):>11}\")\n",
    "    print(f\"{'Accuracy':<12} {accuracy:>11.4f}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# ==================================================\n",
    "# MAIN\n",
    "# ==================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    try:\n",
    "        os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "        os.makedirs(PLOTS_PATH, exist_ok=True)\n",
    "        \n",
    "        X, y, class_names, label_to_idx, X_mean, X_std = load_and_process_data(BASE_DATA_PATH, DESIRED_ROWS)\n",
    "        \n",
    "        print(\"Splitting data...\")\n",
    "        n_samples = X.shape[0]\n",
    "        n_test = int(n_samples * 0.2)\n",
    "        n_val = int(n_samples * 0.1)\n",
    "        \n",
    "        np.random.seed(SEED)\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        train_idx = indices[:n_samples - n_test - n_val]\n",
    "        val_idx = indices[n_samples - n_test - n_val:n_samples - n_test]\n",
    "        test_idx = indices[n_samples - n_test:]\n",
    "        \n",
    "        X_train, y_train = jnp.array(X[train_idx]), jnp.array(y[train_idx])\n",
    "        X_val, y_val = jnp.array(X[val_idx]), jnp.array(y[val_idx])\n",
    "        X_test, y_test = jnp.array(X[test_idx]), jnp.array(y[test_idx])\n",
    "        \n",
    "        print(f\"Train: {len(X_train):,} | Val: {len(X_val):,} | Test: {len(X_test):,}\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        state, best_acc, history = train_model(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            N_EPOCHS, BATCH_SIZE\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"EVALUATION\")\n",
    "        print(\"=\"*70)\n",
    "        test_acc, predictions = eval_step(state, X_test, y_test)\n",
    "        test_acc = float(test_acc)\n",
    "        \n",
    "        print(f\"Training time: {training_time:.1f}s ({training_time/60:.1f} min)\")\n",
    "        print(f\"Test accuracy: {test_acc*100:.2f}%\")\n",
    "        print(f\"NAO control success rate: {test_acc*100:.2f}%\\n\")\n",
    "        \n",
    "        y_pred = np.array(predictions)\n",
    "        y_test_np = np.array(y_test)\n",
    "        \n",
    "        plot_training_curves(history, PLOTS_PATH)\n",
    "        plot_training_history_comprehensive(history, PLOTS_PATH)\n",
    "        cm = plot_confusion_matrix_enhanced(y_test_np, y_pred, class_names, PLOTS_PATH)\n",
    "        plot_per_class_metrics(y_test_np, y_pred, class_names, PLOTS_PATH)\n",
    "        \n",
    "        print_classification_report(y_test_np, y_pred, class_names)\n",
    "        \n",
    "        model_data = {\n",
    "            'params': state.params,\n",
    "            'batch_stats': state.batch_stats,\n",
    "            'class_names': class_names,\n",
    "            'label_to_idx': label_to_idx,\n",
    "            'n_features': X.shape[1],\n",
    "            'hidden_dims': HIDDEN_DIMS,\n",
    "            'n_classes': int(jnp.max(y_train)) + 1,\n",
    "            'X_mean': X_mean,\n",
    "            'X_std': X_std,\n",
    "            'training_time': training_time,\n",
    "            'test_accuracy': test_acc,\n",
    "            'confusion_matrix': cm,\n",
    "            'training_history': history,\n",
    "            'author': 'Yash272001',\n",
    "            'date': '2025-01-19',\n",
    "            'jax_version': jax.__version__\n",
    "        }\n",
    "        \n",
    "        model_path = os.path.join(OUTPUT_PATH, 'jax_deep_learning_model.pkl')\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"\\nModel saved: {model_path}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Generated plots:\")\n",
    "        print(\"  1. training_curves_deep_learning.png\")\n",
    "        print(\"  2. training_history_comprehensive_deep_learning.png\")\n",
    "        print(\"  3. confusion_matrix_deep_learning.png\")\n",
    "        print(\"  4. per_class_metrics_deep_learning.png\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
