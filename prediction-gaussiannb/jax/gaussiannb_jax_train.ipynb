{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab6472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Imports\n",
    "import os, json, pickle, random\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# ============ CONFIGURATION ============\n",
    "BASE_DIR = 'path/to/your/data'  # Replace with actual data directory\n",
    "MOVEMENT_ALIASES = {\n",
    "    'backwards': 'backward',\n",
    "    'backward': 'backward',\n",
    "    'forward': 'forward',\n",
    "    'landing': 'landing',\n",
    "    'left': 'left',\n",
    "    'right': 'right',\n",
    "    'takeoff': 'takeoff',\n",
    "    'take_off': 'takeoff'\n",
    "}\n",
    "KNOWN_MOVEMENTS = set(MOVEMENT_ALIASES.values())\n",
    "RNG_SEED = 42\n",
    "MAX_ROWS_PER_FILE = 1500  # Set to None for all rows\n",
    "TRAIN_RATIO = 0.8\n",
    "VAR_SMOOTHING = 1e-9\n",
    "np.random.seed(RNG_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c2f31",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c08ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def is_movement_dir(path_part: str) -> bool:\n",
    "    canon = MOVEMENT_ALIASES.get(path_part.lower())\n",
    "    return canon in KNOWN_MOVEMENTS if canon else False\n",
    "\n",
    "def canonical_movement(path_part: str) -> str:\n",
    "    return MOVEMENT_ALIASES.get(path_part.lower())\n",
    "\n",
    "def gather_files(base_dir: str) -> Dict[str, List[str]]:\n",
    "    movement_files: Dict[str, List[str]] = {m: [] for m in KNOWN_MOVEMENTS}\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        tail = os.path.basename(root)\n",
    "        if is_movement_dir(tail):\n",
    "            label = canonical_movement(tail)\n",
    "            for f in files:\n",
    "                if f.lower().endswith('.csv'):\n",
    "                    movement_files[label].append(os.path.join(root, f))\n",
    "    return {k: v for k, v in movement_files.items() if v}\n",
    "\n",
    "movement_to_files = gather_files(BASE_DIR)\n",
    "print(f'Found movements: {list(movement_to_files.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_eeg_file(fp: str):\n",
    "    for attempt in ['\\t', ',', 'whitespace']:\n",
    "        try:\n",
    "            if attempt == 'whitespace':\n",
    "                df = pd.read_csv(fp, sep='\\s+', header=None, engine='python')\n",
    "            else:\n",
    "                df = pd.read_csv(fp, sep=attempt, header=None, engine='python')\n",
    "            if df.shape[0] == 0:\n",
    "                continue\n",
    "            arr = df.select_dtypes(include=[float, int]).to_numpy(dtype=np.float32)\n",
    "            if arr.size == 0:\n",
    "                continue\n",
    "            return arr\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def load_samples(movement_files: Dict[str, List[str]], max_rows_per_file: int = MAX_ROWS_PER_FILE):\n",
    "    X_candidates: List[np.ndarray] = []\n",
    "    y_candidates: List[np.ndarray] = []\n",
    "    col_counts: List[int] = []\n",
    "    label_to_index: Dict[str,int] = {}\n",
    "    skipped_fewer, skipped_empty = 0, 0\n",
    "\n",
    "    for label in sorted(movement_files.keys()):\n",
    "        if label not in label_to_index:\n",
    "            label_to_index[label] = len(label_to_index)\n",
    "        idx = label_to_index[label]\n",
    "        for fp in movement_files[label]:\n",
    "            arr = _read_eeg_file(fp)\n",
    "            if arr is None:\n",
    "                skipped_empty += 1\n",
    "                continue\n",
    "            if max_rows_per_file is not None and arr.shape[0] > max_rows_per_file:\n",
    "                arr = arr[:max_rows_per_file]\n",
    "            X_candidates.append(arr)\n",
    "            y_candidates.append(np.full((arr.shape[0],), idx, dtype=np.int32))\n",
    "            col_counts.append(arr.shape[1])\n",
    "\n",
    "    unique, counts = np.unique(np.array(col_counts), return_counts=True)\n",
    "    target_dim = int(unique[np.argmax(counts)])\n",
    "\n",
    "    X_parts, y_parts = [], []\n",
    "    for arr, y_part in zip(X_candidates, y_candidates):\n",
    "        if arr.shape[1] == target_dim:\n",
    "            X_parts.append(arr)\n",
    "            y_parts.append(y_part)\n",
    "        elif arr.shape[1] > target_dim:\n",
    "            X_parts.append(arr[:, :target_dim])\n",
    "            y_parts.append(y_part)\n",
    "        else:\n",
    "            skipped_fewer += 1\n",
    "\n",
    "    X = np.concatenate(X_parts, axis=0)\n",
    "    y = np.concatenate(y_parts, axis=0)\n",
    "    index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "\n",
    "    print(f'Loaded: {X.shape[0]} samples, {target_dim} features, {len(index_to_label)} classes')\n",
    "    return X, y, index_to_label\n",
    "\n",
    "X_raw, y_raw, index_to_label = load_samples(movement_to_files)\n",
    "print('Data shape:', X_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf43f70",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0) + 1e-8\n",
    "    return (X - mean) / std, mean, std\n",
    "\n",
    "# Variance-based filtering + derived statistics + nonlinear transforms\n",
    "raw_var = X_raw.var(axis=0)\n",
    "var_threshold = 1e-3\n",
    "mask = raw_var > var_threshold\n",
    "X_var = X_raw[:, mask]\n",
    "\n",
    "row_mean = X_var.mean(axis=1)\n",
    "row_std = X_var.std(axis=1)\n",
    "row_abs_mean = np.abs(X_var).mean(axis=1)\n",
    "row_energy = np.square(X_var).mean(axis=1)\n",
    "\n",
    "X_abs = np.abs(X_var)\n",
    "X_log1p = np.log1p(np.abs(X_var))\n",
    "\n",
    "X_feat = np.concatenate([X_var, X_abs, X_log1p,\n",
    "                         row_mean[:, None], row_std[:, None],\n",
    "                         row_abs_mean[:, None], row_energy[:, None]], axis=1)\n",
    "\n",
    "X, feat_mean, feat_std = standardize(X_feat)\n",
    "print(f'Engineered features: {X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdda896",
   "metadata": {},
   "source": [
    "## Train/Test Split & Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(X: np.ndarray, y: np.ndarray, train_ratio: float, seed: int = RNG_SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    train_idx, test_idx = [], []\n",
    "    for cls in np.unique(y):\n",
    "        cls_idx = np.where(y == cls)[0]\n",
    "        rng.shuffle(cls_idx)\n",
    "        n_train = int(len(cls_idx) * train_ratio)\n",
    "        train_idx.append(cls_idx[:n_train])\n",
    "        test_idx.append(cls_idx[n_train:])\n",
    "    return X[np.concatenate(train_idx)], y[np.concatenate(train_idx)], X[np.concatenate(test_idx)], y[np.concatenate(test_idx)]\n",
    "\n",
    "X_train, y_train, X_test, y_test = stratified_split(X, y_raw, TRAIN_RATIO)\n",
    "print(f'Train: {X_train.shape}, Test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce879ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNBJAX:\n",
    "    def __init__(self, var_smoothing: float = VAR_SMOOTHING, uniform_priors: bool = True):\n",
    "        self.var_smoothing = float(var_smoothing)\n",
    "        self.uniform_priors = bool(uniform_priors)\n",
    "        self.class_prior_ = None\n",
    "        self.theta_ = None\n",
    "        self.var_ = None\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X_j = jnp.asarray(X, dtype=jnp.float32)\n",
    "        y_j = jnp.asarray(y, dtype=jnp.int32)\n",
    "        num_classes = int(jnp.max(y_j) + 1)\n",
    "        counts = jnp.bincount(y_j, length=num_classes)\n",
    "        counts_f = jnp.maximum(counts.astype(jnp.float32), 1.0)\n",
    "        def sums_for_class(c):\n",
    "            mask = (y_j == c)\n",
    "            masked = jnp.where(mask[:, None], X_j, 0.0)\n",
    "            return jnp.sum(masked, axis=0)\n",
    "        def sums2_for_class(c):\n",
    "            mask = (y_j == c)\n",
    "            masked2 = jnp.where(mask[:, None], X_j * X_j, 0.0)\n",
    "            return jnp.sum(masked2, axis=0)\n",
    "        classes = jnp.arange(num_classes)\n",
    "        sums = jax.vmap(sums_for_class)(classes)\n",
    "        sums2 = jax.vmap(sums2_for_class)(classes)\n",
    "        means = sums / counts_f[:, None]\n",
    "        vars_ = (sums2 / counts_f[:, None]) - jnp.square(means)\n",
    "        vars_ = jnp.maximum(vars_, self.var_smoothing)\n",
    "        if self.uniform_priors:\n",
    "            priors = jnp.ones(num_classes, dtype=jnp.float32) / num_classes\n",
    "        else:\n",
    "            priors = counts.astype(jnp.float32) / float(X_j.shape[0])\n",
    "        self.theta_ = np.asarray(means, dtype=np.float32)\n",
    "        self.var_ = np.asarray(vars_, dtype=np.float32)\n",
    "        self.class_prior_ = np.asarray(priors, dtype=np.float32)\n",
    "        return self\n",
    "    @staticmethod\n",
    "    @jax.jit\n",
    "    def _predict_log_proba_jit(X: jnp.ndarray, mu: jnp.ndarray, var: jnp.ndarray, log_prior: jnp.ndarray) -> jnp.ndarray:\n",
    "        const_term = -0.5 * jnp.sum(jnp.log(2.0 * jnp.pi * var), axis=1)\n",
    "        diff = X[:, None, :] - mu[None, :, :]\n",
    "        quad = -0.5 * jnp.sum((diff * diff) / (var[None, :, :]), axis=2)\n",
    "        log_lik = quad + const_term[None, :]\n",
    "        return log_lik + log_prior[None, :]\n",
    "    def predict_log_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        X_j = jnp.asarray(X, dtype=jnp.float32)\n",
    "        mu = jnp.asarray(self.theta_, dtype=jnp.float32)\n",
    "        var = jnp.asarray(self.var_, dtype=jnp.float32)\n",
    "        log_prior = jnp.log(jnp.asarray(self.class_prior_, dtype=jnp.float32) + 1e-12)\n",
    "        out = self._predict_log_proba_jit(X_j, mu, var, log_prior)\n",
    "        return np.asarray(out)\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return np.argmax(self.predict_log_proba(X), axis=1).astype(np.int32)\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        logp = self.predict_log_proba(X)\n",
    "        m = np.max(logp, axis=1, keepdims=True)\n",
    "        p = np.exp(logp - m)\n",
    "        p /= np.sum(p, axis=1, keepdims=True)\n",
    "        return p\n",
    "\n",
    "model = GaussianNBJAX(var_smoothing=VAR_SMOOTHING, uniform_priors=True)\n",
    "model.fit(X_train, y_train)\n",
    "print('Model trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3870c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa4f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "    C = int(max(y_true.max(), y_pred.max()) + 1)\n",
    "    mat = np.zeros((C, C), dtype=np.int32)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        mat[t, p] += 1\n",
    "    return mat\n",
    "\n",
    "def classification_report(y_true: np.ndarray, y_pred: np.ndarray, index_to_label: Dict[int,str]) -> Dict:\n",
    "    C = int(max(y_true.max(), y_pred.max()) + 1)\n",
    "    report = {}\n",
    "    for c in range(C):\n",
    "        tp = np.sum((y_true == c) & (y_pred == c))\n",
    "        fp = np.sum((y_true != c) & (y_pred == c))\n",
    "        fn = np.sum((y_true == c) & (y_pred != c))\n",
    "        precision = tp / (tp + fp + 1e-12)\n",
    "        recall = tp / (tp + fn + 1e-12)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "        report[index_to_label[c]] = {\n",
    "            'precision': float(precision), 'recall': float(recall),\n",
    "            'f1': float(f1), 'support': int(np.sum(y_true == c))\n",
    "        }\n",
    "    report['overall'] = {'accuracy': float(np.mean(y_true == y_pred))}\n",
    "    return report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, index_to_label)\n",
    "accuracy = report['overall']['accuracy']\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print('\\nPer-class metrics:')\n",
    "for cls, metrics in report.items():\n",
    "    if cls != 'overall':\n",
    "        print(f\"  {cls}: precision={metrics['precision']:.3f}, recall={metrics['recall']:.3f}, f1={metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d5698",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32796a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to pickle\n",
    "model_path = 'gaussiannb_jax_model.pkl'\n",
    "model_metadata = {\n",
    "    'accuracy': accuracy,\n",
    "    'n_train': int(X_train.shape[0]),\n",
    "    'n_test': int(X_test.shape[0]),\n",
    "    'n_features': int(X_train.shape[1]),\n",
    "    'classes': [index_to_label[i] for i in sorted(index_to_label.keys())],\n",
    "    'var_smoothing': float(model.var_smoothing),\n",
    "    'uniform_priors': True\n",
    "}\n",
    "\n",
    "model_state = {\n",
    "    'theta_': model.theta_,\n",
    "    'var_': model.var_,\n",
    "    'class_prior_': model.class_prior_,\n",
    "    'var_smoothing': model.var_smoothing,\n",
    "    'uniform_priors': True,\n",
    "    'metadata': model_metadata,\n",
    "    'index_to_label': index_to_label,\n",
    "    'label_to_index': {v: k for k, v in index_to_label.items()}\n",
    "}\n",
    "\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model_state, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f'Model saved to: {model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258e020",
   "metadata": {},
   "source": [
    "## Load & Verify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aaf0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gaussiannb_jax_model(path: str):\n",
    "    with open(path, 'rb') as f:\n",
    "        state = pickle.load(f)\n",
    "    model_loaded = GaussianNBJAX(\n",
    "        var_smoothing=state['var_smoothing'],\n",
    "        uniform_priors=state['uniform_priors']\n",
    "    )\n",
    "    model_loaded.theta_ = state['theta_']\n",
    "    model_loaded.var_ = state['var_']\n",
    "    model_loaded.class_prior_ = state['class_prior_']\n",
    "    return model_loaded, state\n",
    "\n",
    "model_loaded, state = load_gaussiannb_jax_model(model_path)\n",
    "print(f'Model loaded from {model_path}')\n",
    "print(f'Classes: {state[\"metadata\"][\"classes\"]}')\n",
    "print(f'Accuracy: {state[\"metadata\"][\"accuracy\"]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
